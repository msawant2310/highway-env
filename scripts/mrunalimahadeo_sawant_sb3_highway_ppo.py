# -*- coding: utf-8 -*-
"""MrunaliMahadeo_Sawant_sb3_highway_ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sGpNAMSv21YJ6XH3Tptirv0jVPXc1qcD
"""

#Installing pre-requisite packages
!apt-get update
!pip install gym==0.26
!pip install gym==0.21.0
!pip install highway-env==1.5
!pip install sb3-contrib==1.5.0
!pip install importlib-metadata==4.8.1
# Install environment and agent
!pip install highway-env
# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.
!pip install git+https://github.com/DLR-RM/stable-baselines3

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Environment
import gym
import highway_env

# Agent
from stable_baselines3 import DQN

# Visualization utils
# %load_ext tensorboard
import sys
from tqdm.notebook import trange

#Installing packages for visualization
!pip install tensorboardx gym pyvirtualdisplay
!apt-get install -y xvfb python-opengl ffmpeg
#Cloning repository for use
!git clone https://github.com/eleurent/highway-env.git 2> /dev/null

#Importing and Using functions from the repository
sys.path.insert(0, '/content/highway-env/scripts/')
from utils import record_videos, show_videos

#Impoting installed packages and libraries for model building training and visualization
import gym
import torch as th
from stable_baselines3 import PPO
from torch.distributions import Categorical
import torch
import torch.nn as nn
import numpy as np
from torch.nn import functional as F
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.vec_env import SubprocVecEnv
import highway_env

#Creating environment and training model using reinforcement learning concepts 
#We have also passed required hyper parameters and have trained agents to take actions to maximize rewards
if __name__ == "__main__":
    train = True
    if train:
        n_cpu = 6
        batch_size = 64
        env = make_vec_env("highway-fast-v0", n_envs=n_cpu)
        model = PPO("MlpPolicy",
                    env,
                    policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),
                    n_steps=batch_size * 12 // n_cpu,
                    batch_size=batch_size,
                    n_epochs=10,
                    learning_rate=0.1,
                    gamma=0.8,
                    verbose=2,
                    tensorboard_log="highway_ppo/")
        # Train the agent
        model.learn(total_timesteps=int(2e4))
        # Save the agent
        model.save("highway_ppo/model")

#Using the trained model on the test environment and have obtained essential visualizations
env = gym.make("highway-fast-v0")
env = record_videos(env)
for episode in trange(5, desc="Test episodes"):
    obs, done = env.reset(), False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(int(action))
env.close()
show_videos()
#The agent and learns by keeping rewards constant
#As we can see in the videos the vehicle acts as agent and learns from its past experiences of collision and provides feedbacks to next systems by providing flags and reward status
#The vehicle then avoids moving obstacles by taking actions and keeps/gains constant rewards
#We have also provided different graphs to summarize about the process, required energy, time etc in the interactive dashboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir "highway_ppo"